{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing a Declarative Node using the `ddn.pytorch.node` Module\n",
    "\n",
    "Unlike the previous tutorials, in this notebook we use the [PyTorch](https://pytorch.org/) framework to implement a declarative node. For information on how to use PyTorch, see the [official documentation](https://pytorch.org/docs/stable/index.html) and [tutorials](https://pytorch.org/tutorials/). Here we will show how to implement a declarative node using the `ddn.pytorch.node` module to explore the behavior of the node and solve simple bi-level optimization problems.\n",
    "\n",
    "## Example 1: Minimize the KL-divergence over the probability simplex\n",
    "\n",
    "We consider the problem of minimizing the KL-divergence between the input $x$ and output $y$ subject to the output forming a valid probability vector (i.e., the elements of $y$ be positive and sum to one). We will assume strictly positive $x$. The problem can be written formally as\n",
    "\n",
    "$$\n",
    "\\begin{array}{rll}\n",
    "y =& \\text{argmin}_u & - \\sum_{i=1}^{n} x_i \\log u_i \\\\\n",
    "& \\text{subject to} & \\sum_{i=1}^{n} u_i = 1\n",
    "\\end{array}\n",
    "$$\n",
    "where the positivity constraint on $y$ is automatically satisfied by the domain of the log function.\n",
    "\n",
    "A nice feature of this problem is that we can solve it in closed-form as\n",
    "$$\n",
    "y = \\frac{1}{\\sum_{i=1}^{n} x_i} x.\n",
    "$$\n",
    "\n",
    "However, we will only use this for verification and pretend for now that we do not have a closed-form solution. Instead we will make use of the `scipy.optimize` module to solve the problem via an iterative method. Deriving our deep declarative node from the `LinEqConstDeclarativeNode` class, we will need to implement three functions: the `objective` function, the `solve` function, and the `linear_constraint_parameters` function (the `gradient` function is already implemented for us).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import scipy.optimize as opt\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from ddn.pytorch.node import *\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# create the example node\n",
    "class MinKL(LinEqConstDeclarativeNode):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def objective(self, x, y):\n",
    "        \"\"\"f(x, y) = -sum x*log(y)\"\"\"\n",
    "        return -1.0 * torch.einsum('bn,bn->b', (x, y.log()))\n",
    "    \n",
    "    def linear_constraint_parameters(self, y):\n",
    "        \"\"\"Ay=d ==> sum(y) = 1\"\"\"\n",
    "        A = y.new_ones(1, y.size(-1)) # 1xm\n",
    "        d = y.new_ones(1) # 1\n",
    "        return A, d\n",
    "        \n",
    "    def solve(self, x):\n",
    "        \"\"\"Solve the constrained optimization problem using scipy's built-in minimize function.\n",
    "        Here we initialize the solver at the uniform distribution.\n",
    "        \"\"\"\n",
    "        m = n = x.size(-1)\n",
    "        u0 = np.ones((m,)) / m\n",
    "        y = torch.zeros_like(x)\n",
    "        # Loop over batch:\n",
    "        for i, xi in enumerate(x):\n",
    "            result = opt.minimize(lambda u: -1.0 * np.dot(xi.detach().numpy(), np.log(u)),\n",
    "                                  u0,\n",
    "                                  constraints={'type': 'eq', 'fun': lambda u: np.sum(u) - 1.0},\n",
    "                                  bounds=opt.Bounds(1e-12, np.inf, keep_feasible=True),\n",
    "                                  options={'maxiter': 100000, 'ftol': 1e-12})\n",
    "            y[i, :] = torch.tensor(result.x)\n",
    "        \n",
    "        # The solve function must always return two arguments, the solution and context (i.e., cached values needed\n",
    "        # for computing the gradient). In the case of linearly constrained problems we do not need the dual solution\n",
    "        # in computing the gradient so we return None for context.\n",
    "        return y, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we test the node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node = MinKL()\n",
    "x = torch.rand(1, 5)\n",
    "print(\"Input:\\n{}\".format(x.squeeze().numpy()))\n",
    "print(\"Expected output:\\n{}\".format((x / x.sum(dim=-1, keepdim=True)).squeeze().numpy()))\n",
    "\n",
    "y, _ = node.solve(x)\n",
    "print(\"Actual output:\\n{}\".format(y.squeeze().numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now plot the function and gradient sweeping the first component of the input $x_1$ from 0.1 to 10.0 while holding the other elements of $x$ constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x1_data = torch.linspace(0.1, 10.0, 100)\n",
    "x = x.detach() # Don't track computation graph\n",
    "y_data = []\n",
    "Dy_data = []\n",
    "vjp_data = []\n",
    "for x1 in x1_data:\n",
    "    x_new = x.clone()\n",
    "    x_new[0, 0] = x1\n",
    "    x_new.requires_grad = True\n",
    "    y, _ = torch.no_grad()(node.solve)(x_new) # Run node's forward pass\n",
    "    y.requires_grad = True\n",
    "    y_data.append(y.squeeze().detach().numpy())\n",
    "    # Note that the jacobian function call is inefficient\n",
    "    # and is used only for teaching and analysis purposes\n",
    "    Dy_data.append(node.jacobian(x_new, y=y)[0][0,:,0].detach().numpy())\n",
    "    vjp_data.append(node.gradient(x_new, y=y)[0][0,:].detach().numpy())\n",
    "\n",
    "# Plot output y as x varies\n",
    "fig = plt.figure()\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.plot(x1_data, y_data)\n",
    "plt.ylabel(r\"$y$\")\n",
    "\n",
    "# Plot derivative dy/dx1 as x1 varies\n",
    "# dy/dx = (I - y 1^T) / sum(xi)\n",
    "# dy1/dx1 = (1 - y1) / sum(xi)\n",
    "# dyi/dx1 = -yi / sum(xi), i > 1\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.plot(x1_data, Dy_data)\n",
    "#plt.ylabel(r\"$Dy_{:,1}$\")\n",
    "plt.ylabel(r\"$\\frac{dy}{dx_1}$\")\n",
    "\n",
    "# Plot vector-Jacobian product as x1 varies\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.plot(x1_data, vjp_data)\n",
    "plt.xlabel(r\"$x_1$\");\n",
    "plt.ylabel(r\"$\\mathbf{1}^\\mathsf{T}Dy$\")\n",
    "\n",
    "fig.subplots_adjust(hspace=0.5)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Bi-level optimization\n",
    "\n",
    "Now let's see whether we can use the node within a bi-level optimization problem. We will attempt to learn an input $x$ that results in an output $y$ with smallest norm-squared. Moreover, we will regularize the norm of $x$ to be close to 10. Given our understanding of KL-divergence this should learn a vector $x$ that is a constant multiple of the ones vector (i.e., all elements of $x$ should be the same). Let's see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the upper-level objective\n",
    "def J(x, y=None):\n",
    "    \"\"\"Computes our upper-level objective given both x and y.\"\"\"\n",
    "    if y is None:\n",
    "        y, _ = torch.no_grad()(node.solve)(x)\n",
    "    return ((y.norm(dim=-1)) ** 2 + (x.norm(dim=-1) - 10.0) ** 2).mean()\n",
    "\n",
    "kl_problem = MinKL()\n",
    "kl_declarative_layer = DeclarativeLayer(kl_problem)\n",
    "\n",
    "# Solve using gradient descent:\n",
    "learning_rate = 0.5\n",
    "x = torch.rand(1, 5, requires_grad=True)\n",
    "history = [J(x)]\n",
    "for i in range(500):\n",
    "    y = kl_declarative_layer(x)\n",
    "    z = J(x, y)\n",
    "    z.backward()\n",
    "    x_new = x - learning_rate * x.grad\n",
    "    x = x_new.detach().requires_grad_(True)\n",
    "    history.append(J(x))\n",
    "\n",
    "y, _ = torch.no_grad()(node.solve)(x)\n",
    "x_np = x.detach().squeeze().numpy()\n",
    "y_np = y.detach().squeeze().numpy()\n",
    "print(\"Found x = {} with norm {:0.2f}\".format(x_np, np.sqrt(np.dot(x_np, x_np))))\n",
    "print(\"Results in y = {}\".format(y_np))\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.semilogy(history)\n",
    "plt.ylabel(\"upper-level objective (log-scale)\"); plt.xlabel(\"iteration\")\n",
    "plt.show()\n",
    "\n",
    "# Solve using LBFGS:\n",
    "x = torch.rand(1, 5, requires_grad=True)\n",
    "history = []\n",
    "optimizer = torch.optim.LBFGS([x], lr=1, max_iter=100)\n",
    "def reevaluate():\n",
    "    optimizer.zero_grad()\n",
    "    y = kl_declarative_layer(x)\n",
    "    z = J(x, y)\n",
    "    z.backward()\n",
    "    history.append(z.clone())\n",
    "    return z\n",
    "optimizer.step(reevaluate)\n",
    "\n",
    "y, _ = torch.no_grad()(node.solve)(x)\n",
    "x_np = x.detach().squeeze().numpy()\n",
    "y_np = y.detach().squeeze().numpy()\n",
    "print(\"Found x = {} with norm {:0.2f}\".format(x_np, np.sqrt(np.dot(x_np, x_np))))\n",
    "print(\"Results in y = {}\".format(y_np))\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.semilogy(history)\n",
    "plt.ylabel(\"upper-level objective (log-scale)\"); plt.xlabel(\"iteration\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Minimize a robust (pseudo-Huber) distance\n",
    "\n",
    "We consider the problem of minimizing the distance between the input $x$ and output $y$ using the robust pseudo-Huber penalty function. The problem can be written formally as\n",
    "$$\n",
    "\\begin{equation}\n",
    "y = \\text{argmin}_u \\sum_{i=1}^{n} \\phi^\\text{pseudo}(u - x_i; \\alpha)\n",
    "\\end{equation}\n",
    "$$\n",
    "where the pseudo-Huber penalty function is given by\n",
    "$$\n",
    "\\begin{equation}\n",
    "  \\phi^{\\text{pseudo}}(z; \\alpha) = \\alpha^2 \\left( \\sqrt{1 + \\left(\\frac{z}{\\alpha}\\right)^2} - 1 \\right).\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Deriving our deep declarative node from the `AbstractDeclarativeNode` class, we will need to implement two functions: the `objective` function, and the `solve` function. However, we will also provide a `gradient` function to compare the generic gradient result with an efficient hand-coded gradient that makes use of the structure of the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from ddn.pytorch.node import *\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class GlobalPseudoHuberPool2d(AbstractDeclarativeNode):\n",
    "    \"\"\"\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def objective(self, x, alpha, y):\n",
    "        alpha2 = alpha * alpha\n",
    "        z = y.unsqueeze(-1).unsqueeze(-1) - x\n",
    "        phi = alpha2 * (torch.sqrt(1.0 + torch.pow(z, 2) / alpha2) - 1.0)\n",
    "        return phi.sum(dim=(-2,-1)) # b\n",
    "\n",
    "    def solve(self, x, alpha):\n",
    "        x = x.detach()\n",
    "        y = x.mean([-2, -1]).clone().requires_grad_()\n",
    "        y = self._runOptimisation(x, alpha, y)\n",
    "        y = y.detach()\n",
    "        z = (y.unsqueeze(-1).unsqueeze(-1) - x).clone()\n",
    "        ctx = {'z': z}\n",
    "        return y, ctx\n",
    "    \n",
    "    def _runOptimisation(self, x, alpha, y):\n",
    "        with torch.enable_grad():\n",
    "            opt = torch.optim.LBFGS([y],\n",
    "                                    lr=1, # Default: 1\n",
    "                                    max_iter=100, # Default: 20\n",
    "                                    max_eval=None, # Default: None\n",
    "                                    tolerance_grad=1e-05, # Default: 1e-05\n",
    "                                    tolerance_change=1e-09, # Default: 1e-09\n",
    "                                    history_size=100, # Default: 100\n",
    "                                    line_search_fn=None # Default: None, Alternative: \"strong_wolfe\"\n",
    "                                    )\n",
    "            def reevaluate():\n",
    "                opt.zero_grad()\n",
    "                f = self.objective(x, alpha, y).sum() # sum over batch elements\n",
    "                f.backward()\n",
    "                return f\n",
    "            opt.step(reevaluate)\n",
    "        return y\n",
    "\n",
    "    def gradient(self, x, alpha, y=None, v=None, ctx=None):\n",
    "        \"\"\"Override base class to compute the analytic gradient of the optimal solution.\"\"\"\n",
    "        if y is None:\n",
    "            y, ctx = torch.no_grad()(self.solve)(x, alpha)\n",
    "        if v is None:\n",
    "            v = torch.ones_like(y)\n",
    "        z = ctx['z'] # b x n1 x n2\n",
    "        w = torch.pow(1.0 + torch.pow(z, 2) / (alpha * alpha), -1.5)\n",
    "        w_sum = w.sum(dim=-1, keepdim=True).sum(dim=-2, keepdim=True).expand_as(w)\n",
    "        Dy_at_x = torch.where(w_sum.abs() <= 1e-9, torch.zeros_like(w), w.div(w_sum))  # b x n1 x n2\n",
    "        return torch.einsum('b,bmn->bmn', (v, Dy_at_x)), None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we test the node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node = GlobalPseudoHuberPool2d()\n",
    "batch_size = 3\n",
    "input_size = (6, 6)\n",
    "x = torch.randn(batch_size, *input_size, dtype=torch.double, requires_grad=True)\n",
    "alpha = torch.tensor([0.5], dtype=torch.double, requires_grad=False)\n",
    "y, _ = torch.no_grad()(node.solve)(x, alpha)\n",
    "\n",
    "print(\"Input:\\n{}\".format(x[0,...].squeeze().detach().numpy())) # First batch element only\n",
    "print(\"Output:\\n{}\".format(y[0,...].squeeze().detach().numpy())) # First batch element only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now plot the function and gradient sweeping the first component of the input $x_1$ from -10.0 to 10.0 while holding the other elements of $x$ constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x1_data = torch.linspace(-10.0, 10.0, 110)\n",
    "x = x.detach() # Don't track computation graph\n",
    "y_data = []\n",
    "vjp_data = []\n",
    "vjp2_data = []\n",
    "for x1 in x1_data:\n",
    "    x_new = x.clone()\n",
    "    x_new[:, 0, 0] = x1\n",
    "    x_new.requires_grad = True\n",
    "    y, ctx = torch.no_grad()(node.solve)(x_new, alpha)\n",
    "    y.requires_grad = True\n",
    "    y_data.append(y[0,...].squeeze().detach().numpy()) # First batch element only\n",
    "    vjp_data.append(super(type(node), node).gradient(x_new, alpha, y=y, ctx=ctx)[0][0,0,:].detach().numpy()) # First 6 components\n",
    "    vjp2_data.append(node.gradient(x_new, alpha, y=y, ctx=ctx)[0][0,0,:].detach().numpy()) # First 6 components\n",
    "    \n",
    "fig = plt.figure()\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.plot(x1_data, y_data)\n",
    "plt.ylabel(r\"$y$\")\n",
    "\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.plot(x1_data, vjp_data)\n",
    "plt.xlabel(r\"$x_1$\"); plt.ylabel(r\"$\\mathbf{1}^\\mathsf{T}Dy$ (generic)\")\n",
    "\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.plot(x1_data, vjp2_data)\n",
    "plt.xlabel(r\"$x_1$\"); plt.ylabel(r\"$\\mathbf{1}^\\mathsf{T}Dy$ (analytic)\")\n",
    "\n",
    "fig.subplots_adjust(hspace=0.5)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3: Minimize a PnP objective function\n",
    "\n",
    "We consider the problem of minimizing the weighted reprojection error between a set of corresponding 3D and 2D points $\\{p_i, q_i \\}_{i=1}^n$ by varying the rigid transformation parameters $y$ applied to the 3D points. Here the transformation parameters consist of an angle-axis rotation vector concatenated with a translation vector. The problem can be written formally as\n",
    "$$\n",
    "y = \\text{argmin}_u \\sum_{i=1}^{n} w_i \\| \\pi(p_i, u) - q_i \\|_2^2\n",
    "$$\n",
    "where the projection $\\pi(\\cdot)$ is given by\n",
    "$$\n",
    "\\pi(p, u) = h(K (R(u) p + t(u)))\n",
    "$$\n",
    "with intrinsic camera parameters $K$, rotation $R$, translation $t$, and map from homogeneous-to-Cartesian coordinates $h$, where $h(x) = [x_1 / x_3, x_2 / x_3]$.\n",
    "\n",
    "Deriving our deep declarative node from the `AbstractDeclarativeNode` class, we will need to implement two functions: the `objective` function, and the `solve` function. For this class, we use the `solvePnPRansac` function from the Python OpenCV library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import sys\n",
    "import cv2 as cv\n",
    "from math import degrees\n",
    "sys.path.append(\"../\")\n",
    "from ddn.pytorch.node import *\n",
    "import ddn.pytorch.geometry_utilities as geo\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class PnP(AbstractDeclarativeNode):\n",
    "    \"\"\"Declarative PnP layer\"\"\"\n",
    "    def __init__(self,\n",
    "                 ransac_threshold=0.1,\n",
    "                 ransac_max_iterations=1000,\n",
    "                ):\n",
    "        super().__init__()\n",
    "        self.ransac_threshold = ransac_threshold\n",
    "        self.ransac_max_iterations = ransac_max_iterations\n",
    "        \n",
    "    def objective(self, p, q, w, K, y):\n",
    "        \"\"\"Weighted reprojection error\"\"\"\n",
    "        p_projected = geo.project_points_by_theta(p, y, K)\n",
    "        squared_error = torch.sum((p_projected - q) ** 2, dim=-1)\n",
    "        w = torch.nn.functional.relu(w) # Enforce non-negative weights\n",
    "        return torch.einsum('bn,bn->b', (w, squared_error))\n",
    "    \n",
    "    def solve(self, p, q, w, K=None):\n",
    "        p = p.detach()\n",
    "        q = q.detach()\n",
    "        w = w.detach()\n",
    "        K = K.detach() if K is not None else None\n",
    "        y = self._initialise_transformation(p, q, w, K).requires_grad_()\n",
    "        y = self._run_optimisation(p, q, w, K, y=y)\n",
    "        return y.detach(), None\n",
    "    \n",
    "    def _ransac_p3p(self, p, q, K, threshold, max_iterations):\n",
    "        p_np = p.cpu().numpy()\n",
    "        q_np = q.cpu().numpy()\n",
    "        y = q.new_zeros(q.size(0), 6)\n",
    "        if K is None:\n",
    "            K_np = np.float32(np.array([[1.0, 0.0, 0.0],\n",
    "                                        [0.0, 1.0, 0.0],\n",
    "                                        [0.0, 0.0, 1.0]]))\n",
    "        for i in range(q_np.shape[0]): # loop over batch\n",
    "            if K is not None:\n",
    "                K_np = np.float32(np.array([[K[i, 0], 0.0, K[i, 2]],\n",
    "                                            [0.0, K[i, 1], K[i, 3]],\n",
    "                                            [0.0, 0.0, 1.0]]))\n",
    "            retval, rvec, tvec, inliers = cv.solvePnPRansac(\n",
    "                p_np[i, :, :], q_np[i, :, :], K_np, None,\n",
    "                iterationsCount=max_iterations,\n",
    "                reprojectionError=threshold,\n",
    "                flags=cv.SOLVEPNP_EPNP)\n",
    "            if rvec is not None and tvec is not None and retval:\n",
    "                rvec = torch.as_tensor(rvec, dtype=q.dtype, device=q.device).squeeze(-1)\n",
    "                tvec = torch.as_tensor(tvec, dtype=q.dtype, device=q.device).squeeze(-1)\n",
    "                if torch.isfinite(rvec).all() and torch.isfinite(tvec).all():\n",
    "                    y[i, :3] = rvec\n",
    "                    y[i, 3:] = tvec\n",
    "        return y\n",
    "    \n",
    "    def _initialise_transformation(self, p, q, w, K):\n",
    "        return self._ransac_p3p(p, q, K, self.ransac_threshold, self.ransac_max_iterations)\n",
    "\n",
    "    def _run_optimisation(self, *xs, y):\n",
    "        with torch.enable_grad():\n",
    "            opt = torch.optim.LBFGS([y],\n",
    "                                    lr=1.0,\n",
    "                                    max_iter=1000,\n",
    "                                    max_eval=None,\n",
    "                                    tolerance_grad=1e-40,\n",
    "                                    tolerance_change=1e-40,\n",
    "                                    history_size=100,\n",
    "                                    line_search_fn=\"strong_wolfe\"\n",
    "                                    )\n",
    "            def reevaluate():\n",
    "                opt.zero_grad()\n",
    "                f = self.objective(*xs, y=y).sum() # sum over batch elements\n",
    "                f.backward()\n",
    "                return f\n",
    "            opt.step(reevaluate)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we test the node with 15 random 2D-3D point pairs, random camera parameters, significant additive Gaussian noise, and a single outlier correspondence. We should expect poor results for PnP algorithms when there are outliers, but perhaps we can learn to identify such outliers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node = PnP()\n",
    "b = 1\n",
    "n = 15\n",
    "\n",
    "# Generate camera parameters:\n",
    "y_true = torch.randn(b, 6, dtype=torch.double)\n",
    "R_true = geo.angle_axis_to_rotation_matrix(y_true[..., :3])\n",
    "t_true = y_true[..., 3:]\n",
    "\n",
    "# Generate image points, then assign depths:\n",
    "xy = 2.0 * torch.rand(b, n, 2, dtype=torch.double) - 1.0 # [-1, 1]\n",
    "z = 2.0 * torch.rand(b, n, 1, dtype=torch.double) + 1.0 # [1, 3]\n",
    "p_transformed = torch.cat((z * xy, z), dim=-1)\n",
    "p = torch.einsum('brs,bms->bmr', (R_true.transpose(-2,-1), p_transformed - t_true.unsqueeze(-2))) # Inverse transform\n",
    "q = xy.clone()\n",
    "q = q + 0.1 * torch.randn(b, n, 2, dtype=torch.double) # add noise\n",
    "q[:, 0:1, :] = torch.randn(b, 1, 2, dtype=torch.double) # add outliers\n",
    "\n",
    "# Generate weights (uniform):\n",
    "w = torch.ones(b, n, dtype=torch.double) # bxn\n",
    "w = w.div(w.sum(-1).unsqueeze(-1))\n",
    "\n",
    "# Run solver:\n",
    "y, _ = torch.no_grad()(node.solve)(p, q, w)\n",
    "R = geo.angle_axis_to_rotation_matrix(y[..., :3])\n",
    "t = y[..., 3:]\n",
    "\n",
    "# Compute objective function value:\n",
    "reproj_error = torch.no_grad()(node.objective)(p, q, w, K=None, y=y)\n",
    "reproj_error_true = torch.no_grad()(node.objective)(p, q, w, K=None, y=y_true)\n",
    "\n",
    "# Compute transformation errors:\n",
    "error_rotation = (0.5 * ((R * R_true).sum(dim=(-2, -1)) - 1.0)).acos()\n",
    "error_translation = (t - t_true).norm(dim=-1)\n",
    "\n",
    "# Save original data:\n",
    "p_orig = p.clone()\n",
    "q_orig = q.clone()\n",
    "w_orig = w.clone()\n",
    "y_orig = y.clone()\n",
    "\n",
    "print(\"True Output:\\n{}\".format(y_true[0,...].squeeze().detach().numpy())) # First batch element only\n",
    "print(\"Est. Output:\\n{}\".format(y[0,...].squeeze().detach().numpy())) # First batch element only\n",
    "print(\"True Reprojection Error: {:0.4f}\".format(reproj_error_true[0,...].squeeze().detach().numpy())) # First batch element only\n",
    "print(\"Est. Reprojection Error: {:0.4f}\".format(reproj_error[0,...].squeeze().detach().numpy())) # First batch element only\n",
    "print(\"Rotation Error: {:0.2f} degrees\".format(degrees(error_rotation[0,...].squeeze().detach().numpy()))) # First batch element only\n",
    "print(\"Translation Error: {:0.2f}\".format(error_translation[0,...].squeeze().detach().numpy())) # First batch element only\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clear that even a single outlier can play havoc with PnP estimation. We can visualize this by plotting the 2D points and projected 3D points, using the true and estimated transformation parameters. We link the putative 2D and 3D correspondences with a line, to make the outlier correspondence clear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "q_np = q.numpy()\n",
    "p_proj_true_np = geo.project_points_by_theta(p, y_true).numpy()\n",
    "p_proj_np = geo.project_points_by_theta(p, y).numpy()\n",
    "for i in range(q_np[0, :, 0].shape[0]):\n",
    "   plt.plot([q_np[0, :, 0], p_proj_true_np[0, :, 0]], [q_np[0, :, 1], p_proj_true_np[0, :, 1]], color='gray', linewidth=0.5)  \n",
    "plt.scatter(q_np[0, :, 0], q_np[0, :, 1], s=16, c='k', alpha=1.0, marker='s', label='2D points')\n",
    "plt.scatter(p_proj_true_np[0, :, 0], p_proj_true_np[0, :, 1], s=16, c='r', alpha=1.0, marker='o', label='3D points (true projection)')\n",
    "plt.scatter(p_proj_np[0, :, 0], p_proj_np[0, :, 1], s=16, facecolors='none', edgecolors='k', alpha=1.0, marker='o', label='3D points (est. projection)')\n",
    "plt.legend(fontsize='small')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bi-level optimization\n",
    "\n",
    "Now let's try to learn weights $w$ that attenuate the effect of the outlier correspondences, including those that occur due to noise. Our upper-level objective function will be a weighted sum of rotation and translation errors, given that we know the true camera pose. We expect the outlier correspondence to be downweighted, as well as some of the noisier points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the upper-level objective:\n",
    "def J(p, q, w, y=None):\n",
    "    \"\"\"Compute sum of angular and positional camera errors\"\"\"\n",
    "    if y is None:\n",
    "        y, _ = torch.no_grad()(node.solve)(p, q, w)\n",
    "    R = geo.angle_axis_to_rotation_matrix(y[..., :3])\n",
    "    t = y[..., 3:]\n",
    "    max_dot_product = 1.0 - 1e-7\n",
    "    error_rotation = (0.5 * ((R * R_true).sum(dim=(-2, -1)) - 1.0)\n",
    "                     ).clamp_(-max_dot_product, max_dot_product).acos()\n",
    "    error_translation = (t - t_true).norm(dim=-1)\n",
    "    #print(\"rot: {:0.2f}, trans: {:0.6f}\".format(degrees(error_rotation[0,...]), error_translation[0,...]))\n",
    "    return (error_rotation + 0.25 * error_translation).mean(), error_rotation, error_translation\n",
    "\n",
    "# Reset parameters:\n",
    "w = w_orig.clone().detach().requires_grad_()\n",
    "y = y_orig.clone()\n",
    "\n",
    "# Form a declarative layer:\n",
    "pnp_declarative_layer = DeclarativeLayer(node)\n",
    "\n",
    "loss, error_rotation, error_translation = J(p, q, w, y)\n",
    "history_loss = [loss]\n",
    "history_rot = [degrees(error_rotation[0, ...])] # First batch element only\n",
    "history_tran = [error_translation[0, ...]] # First batch element only\n",
    "\n",
    "# Solve using LBFGS optimizer:\n",
    "optimizer = torch.optim.LBFGS([w], lr=1, max_iter=50, line_search_fn=\"strong_wolfe\")\n",
    "def reevaluate():\n",
    "    optimizer.zero_grad()\n",
    "    y = pnp_declarative_layer(p, q, w, None)\n",
    "    z, error_rotation, error_translation = J(p, q, w, y)\n",
    "    z.backward()\n",
    "    history_loss.append(z.clone())\n",
    "    history_rot.append(degrees(error_rotation[0, ...])) # First batch element only\n",
    "    history_tran.append(error_translation[0, ...]) # First batch element only\n",
    "    return z\n",
    "optimizer.step(reevaluate)\n",
    "\n",
    "w = torch.nn.functional.relu(w) # Enforce non-negativity\n",
    "\n",
    "y, _ = torch.no_grad()(node.solve)(p, q, w)\n",
    "R = geo.angle_axis_to_rotation_matrix(y[..., :3])\n",
    "t = y[..., 3:]\n",
    "reproj_error = torch.no_grad()(node.objective)(p, q, w, K=None, y=y)\n",
    "error_rotation = (0.5 * ((R * R_true).sum(dim=(-2, -1)) - 1.0)).acos()\n",
    "error_translation = (t - t_true).norm(dim=-1)\n",
    "\n",
    "p_np = p.detach().numpy()\n",
    "q_np = q.detach().numpy()\n",
    "w_np = w.detach().numpy()\n",
    "y_np = y.detach().numpy()\n",
    "\n",
    "print(\"Found w = {}\".format(w_np[0, ...]))\n",
    "print(\"Reprojection Error: {:0.4f}\".format(reproj_error[0,...].squeeze().detach().numpy()))\n",
    "print(\"Rotation Error: {:0.2f} degrees\".format(degrees(error_rotation[0,...].squeeze().detach().numpy())))\n",
    "print(\"Translation Error: {:0.6f}\".format(error_translation[0,...].squeeze().detach().numpy()))\n",
    "print(\"True Output: {}\".format(y_true[0,...].squeeze().detach().numpy())) # First batch element only\n",
    "print(\"Est. Output: {}\".format(y[0,...].squeeze().detach().numpy())) # First batch element only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we plot the learning curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.plot(history_loss)\n",
    "plt.ylabel(\"upper-level objective\"); plt.xlabel(\"iteration\")\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.plot(history_rot)\n",
    "plt.ylabel(\"rotation error (degrees)\"); plt.xlabel(\"iteration\")\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.plot(history_tran)\n",
    "plt.ylabel(\"translation error\"); plt.xlabel(\"iteration\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize the results by plotting the 2D points and projected 3D points. We scale the points by the estimated weight, and replace points with weight $\\approx 0$ with crosses to indicate outlier correspondences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "p_proj_true_np = geo.project_points_by_theta(p.detach(), y_true).numpy()\n",
    "p_proj_np = geo.project_points_by_theta(p.detach(), y).numpy()\n",
    "\n",
    "for i in range(q_np[0, :, 0].shape[0]):\n",
    "#     plt.plot([q_np[0, :, 0], p_proj_true_np[0, :, 0]], [q_np[0, :, 1], p_proj_true_np[0, :, 1]], color='gray', linewidth=0.5)\n",
    "    plt.plot([q_np[0, :, 0], p_proj_np[0, :, 0]], [q_np[0, :, 1], p_proj_np[0, :, 1]], color='gray', linewidth=0.5)  \n",
    "plt.scatter(q_np[0, :, 0], q_np[0, :, 1], s=200.*w_np[0,...], c='k', alpha=1.0, marker='s', label='2D points')\n",
    "plt.scatter(p_proj_true_np[0, :, 0], p_proj_true_np[0, :, 1], s=200.*w_np[0,...], c='r', alpha=1.0, marker='o', label='3D points (true projection)')\n",
    "plt.scatter(p_proj_np[0, :, 0], p_proj_np[0, :, 1], s=200.*w_np[0,...], facecolors='none', edgecolors='k', alpha=1.0, marker='o', label='3D points (est. projection)')\n",
    "# Plot identified outliers separately:\n",
    "plt.scatter(q_np[0, w_np[0,...] < 1e-3, 0], q_np[0, w_np[0,...] < 1e-3, 1], s=16, c='k', alpha=1.0, marker='x', label='2D points (outliers)')\n",
    "plt.scatter(p_proj_true_np[0, w_np[0,...] < 1e-3, 0], p_proj_true_np[0, w_np[0,...] < 1e-3, 1], s=16, c='k', alpha=1.0, marker='x', label='3D points (outliers)')\n",
    "plt.scatter(p_proj_np[0, w_np[0,...] < 1e-3, 0], p_proj_np[0, w_np[0,...] < 1e-3, 1], s=16, c='k', alpha=1.0, marker='x', label='3D points (outliers)')\n",
    "plt.legend(fontsize='small')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
